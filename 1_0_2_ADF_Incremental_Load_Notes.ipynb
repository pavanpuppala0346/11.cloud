{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4ed311",
   "metadata": {},
   "source": [
    "# Azure Data Factory - Incremental Data Load\n",
    "This notebook explains how to implement an **Incremental Data Load** in Azure Data Factory (ADF) using a practical pipeline example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746781-1365-4e5f-8b9d-df2d6a73f0ab",
   "metadata": {},
   "source": [
    "#### üìå Business Requirement\n",
    "we have data comming to our order db on daily basis in sql db and we have to create one pipeline which will be able to transfer data on daily basis and incremental way with limited column in another table\n",
    "EX: if 5 rows copy to storage account so next timeif 6th row present then it should ccopy only 6th row\n",
    "\n",
    "- Data is stored in a SQL DB `orders` table.\n",
    "- We need to copy data to a `orders_final` table **incrementally**.\n",
    "- Only a subset of columns should be copied (e.g., exclude `last_name`).\n",
    "- On re-run, only new records (based on `insert_time`) should be copied.\n",
    "\n",
    "#### üîÑ What is Incremental Load?\n",
    "- Rather than loading full data every time, we copy **only new records**.\n",
    "- Helps reduce data load, improves efficiency and performance.\n",
    "\n",
    "#### üß† Strategy\n",
    "1. Extract the maximum `insert_time` from the **sink** table (`orders_final`).\n",
    "2. In the next load, query only records from the **source** table (`orders`) with `insert_time > max_insert_time`.\n",
    "3. Copy only relevant columns to the final table.\n",
    "\n",
    "#### üõ†Ô∏è ADF Pipeline Steps\n",
    "1. **Lookup Activity**: Get the latest `insert_time` from `orders_final` table.\n",
    "   ```sql\n",
    "   SELECT MAX(insert_time) AS date1 FROM orders_final\n",
    "   ```\n",
    "2. **Copy Activity**:\n",
    "   - Source: SQL table `orders`\n",
    "   - Query:\n",
    "   ```sql\n",
    "   SELECT order_id, name, insert_time\n",
    "   FROM orders\n",
    "   WHERE insert_time > '@{activity('Lookup1').output.firstRow.date1}'\n",
    "   ```\n",
    "   - Sink: SQL table `orders_final`\n",
    "  \n",
    "#### ‚úÖ Sample Tables\n",
    "#### Source: `orders`\n",
    "| order_id | name     | last_name | insert_time          |\n",
    "|----------|----------|-----------|----------------------|\n",
    "| 1        | Alice    | Smith     | 2025-07-25 09:10:00  |\n",
    "| 2        | Bob      | Johnson   | 2025-07-25 09:15:00  |\n",
    "| 3        | Charlie  | Brown     | 2025-07-25 09:20:00  |\n",
    "| 4        | Diana    | Prince    | 2025-07-26 10:00:00  |\n",
    "\n",
    "#### Sink: `orders_final`\n",
    "| order_id | name     | insert_time          |\n",
    "|----------|----------|----------------------|\n",
    "| 1        | Alice    | 2025-07-25 09:10:00  |\n",
    "| 2        | Bob      | 2025-07-25 09:15:00  |\n",
    "| 3        | Charlie  | 2025-07-25 09:20:00  |\n",
    "\n",
    "#### üîÅ Run and Verify\n",
    "- Insert new record in `orders` table.\n",
    "- Run the pipeline again.\n",
    "- Verify that only new records are inserted into `orders_final`.\n",
    "- Example:\n",
    "```sql\n",
    "INSERT INTO orders VALUES (5, 'Eve', 'Evans', '2025-07-28 17:00:00');\n",
    "```\n",
    "Expected: Only record with order_id=5 should be inserted.\n",
    "\n",
    "#### üìù Notes\n",
    "- Always use a reliable timestamp (`insert_time`) for incremental logic.\n",
    "- Make sure timezone and format consistency is maintained.\n",
    "- Consider watermark tables for production-grade pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c777b99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29f016fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e59e89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5a2c78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51b3a06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ae5c64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c22c9-cb77-4b4b-a611-cba9103102b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d937d-028c-42c0-bf39-537a96a4547c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0eec37-a0d7-4080-928b-bf3e34cd84c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18930cb1-1cfe-4d81-9ead-4ac2669fe058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470471fa-b38a-461a-a2b4-34b03fb1c401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
